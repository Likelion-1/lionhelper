#!/usr/bin/env python3
"""
Ollama GPT-OSS-20B ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import requests
import json

def test_ollama_connection():
    """Ollama ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        # Ollama ì„œë²„ ìƒíƒœ í™•ì¸
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            print("âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ")
            models = response.json()
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {[model['name'] for model in models.get('models', [])]}")
            return True
        else:
            print("âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
            return False
    except Exception as e:
        print(f"âŒ Ollama ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {e}")
        return False

def test_ollama_generation():
    """Ollama ëª¨ë¸ ìƒì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        url = "http://localhost:11434/api/generate"
        
        payload = {
            "model": "gpt-oss:20b",
            "prompt": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ì„ í•´ì£¼ì„¸ìš”.",
            "stream": False,
            "options": {
                "num_predict": 100,
                "temperature": 0.7
            }
        }
        
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        
        result = response.json()
        print("âœ… Ollama ëª¨ë¸ ìƒì„± ì„±ê³µ")
        print(f"ì‘ë‹µ: {result.get('response', 'ì‘ë‹µ ì—†ìŒ')}")
        return True
        
    except Exception as e:
        print(f"âŒ Ollama ëª¨ë¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return False

def test_fastapi_integration():
    """FastAPIì™€ì˜ í†µí•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        url = "http://localhost:8000/chat"
        
        payload = {
            "prompt": "ì‚¬ë‘ë‹ˆ ë°œì¹˜ ê´€ë ¨í•´ì„œ ì§ˆë¬¸ì´ ìˆì–´ìš”",
            "use_ollama": True
        }
        
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        
        result = response.json()
        print("âœ… FastAPI í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        print(f"ì‘ë‹µ: {result.get('response', 'ì‘ë‹µ ì—†ìŒ')}")
        print(f"ëª¨ë¸: {result.get('model', 'ëª¨ë¸ ì •ë³´ ì—†ìŒ')}")
        print(f"ì‘ë‹µ íƒ€ì…: {result.get('response_type', 'íƒ€ì… ì •ë³´ ì—†ìŒ')}")
        return True
        
    except Exception as e:
        print(f"âŒ FastAPI í†µí•© í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Ollama GPT-OSS-20B í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # 1. Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
    print("\n1. Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸")
    if not test_ollama_connection():
        print("Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        print("ollama serve")
        exit(1)
    
    # 2. Ollama ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸
    print("\n2. Ollama ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    if not test_ollama_generation():
        print("ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:")
        print("ollama pull openai/gpt-oss-20b")
        exit(1)
    
    # 3. FastAPI í†µí•© í…ŒìŠ¤íŠ¸
    print("\n3. FastAPI í†µí•© í…ŒìŠ¤íŠ¸")
    test_fastapi_integration()
    
    print("\n" + "=" * 50)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

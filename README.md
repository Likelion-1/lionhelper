# Llama-3 Korean Chat API

FastAPI κΈ°λ°μ Llama-3 ν•κµ­μ–΄ μ±—λ΄‡ μ›Ή μ„λΉ„μ¤μ…λ‹λ‹¤. MLP-KTLim/llama-3-Korean-Bllossom-8B λ¨λΈμ„ μ‚¬μ©ν•©λ‹λ‹¤.

## π μ¨λΌμΈ λ°λ¨

- **μ›Ή μΈν„°νμ΄μ¤**: λΈλΌμ°μ €μ—μ„ λ°”λ΅ μ‚¬μ© κ°€λ¥ν• μ±„ν… μΈν„°νμ΄μ¤
- **API μ—”λ“ν¬μΈνΈ**: λ‹¤λ¥Έ μ• ν”λ¦¬μΌ€μ΄μ…μ—μ„ μ‚¬μ©ν•  μ μλ” REST API
- **μλ™ λ¬Έμ„**: `/docs`μ—μ„ Swagger UIλ΅ API λ¬Έμ„ ν™•μΈ
- **λ¨λΈ**: MLP-KTLim/llama-3-Korean-Bllossom-8B (8B νλΌλ―Έν„°)

## π€ μ¥μ 

- **λ©€ν‹° μ‘μ—… μ§€μ›**: μ—¬λ¬ μ‚¬μ©μκ°€ λ™μ‹μ— μ‚¬μ© κ°€λ¥
- **ν¬λ΅μ¤ ν”λ«νΌ**: Windows, macOS, Linuxμ—μ„ λ™μΌν•κ² μ‘λ™
- **λ¦¬μ†μ¤ ν¨μ¨μ„±**: Ollamaκ°€ λ¨λΈμ„ λ³„λ„λ΅ κ΄€λ¦¬
- **ν™•μ¥μ„±**: μ›Ή APIλ΅ λ‹¤μ–‘ν• ν΄λΌμ΄μ–ΈνΈμ—μ„ μ ‘κ·Ό κ°€λ¥

## π“‹ μ‚¬μ „ μ”κµ¬μ‚¬ν•­

1. **Ollama μ„¤μΉ**
   - [Ollama κ³µμ‹ μ‚¬μ΄νΈ](https://ollama.ai/)μ—μ„ λ‹¤μ΄λ΅λ“
   - Windows: `winget install Ollama.Ollama`
   - macOS: `brew install ollama`

2. **λ¨λΈ λ‹¤μ΄λ΅λ“**
   ```bash
   ollama pull mayo/llama-3-korean-bllossom-8b
   ```

## π› οΈ λ΅μ»¬ μ„¤μΉ λ° μ‹¤ν–‰

1. **μμ΅΄μ„± μ„¤μΉ**
   ```bash
   pip install -r requirements.txt
   ```

2. **FastAPI μ„λ²„ μ‹¤ν–‰**
   ```bash
   python main.py
   # λλ”
   uvicorn main:app --reload --host 0.0.0.0 --port 8000
   ```

3. **μ›Ή λΈλΌμ°μ €μ—μ„ μ ‘μ†**
   ```
   http://localhost:8000
   ```

**μ°Έκ³ **: μ²« μ‹¤ν–‰ μ‹ λ¨λΈ λ‹¤μ΄λ΅λ“μ— μ‹κ°„μ΄ κ±Έλ¦΄ μ μμµλ‹λ‹¤ (μ•½ 15-20GB).

## π€ λ¬΄λ£ νΈμ¤ν… λ°°ν¬

### Render.com λ°°ν¬ (κ¶μ¥)

1. **GitHubμ— μ½”λ“ μ—…λ΅λ“**
   ```bash
   git init
   git add .
   git commit -m "Initial commit"
   git remote add origin https://github.com/yourusername/llama3-korean-chat.git
   git push -u origin main
   ```

2. **Render.comμ—μ„ μƒ μ„λΉ„μ¤ μƒμ„±**
   - [Render.com](https://render.com)μ— κ°€μ…
   - "New Web Service" ν΄λ¦­
   - GitHub μ €μ¥μ† μ—°κ²°
   - ν™κ²½ λ³€μ μ„¤μ •:
     - `OLLAMA_API_URL`: μ‹¤μ  Ollama μ„λ²„ URL
     - `MODEL_NAME`: μ‚¬μ©ν•  λ¨λΈλ…

3. **λ°°ν¬ μ™„λ£**
   - μλ™μΌλ΅ λ°°ν¬κ°€ μ‹μ‘λ©λ‹λ‹¤
   - λ°°ν¬ μ™„λ£ ν›„ μ κ³µλλ” URLλ΅ μ ‘μ†

### λ‹¤λ¥Έ ν”λ«νΌ

- **Heroku**: `Procfile` μ‚¬μ©
- **Railway**: `railway.json` μ„¤μ •
- **Vercel**: `vercel.json` μ„¤μ •

## π API μ—”λ“ν¬μΈνΈ

- `GET /`: μ„λ²„ μ •λ³΄
- `POST /chat`: μ±—λ΄‡ λ€ν™”
- `GET /health`: μ„λ²„ μƒνƒ ν™•μΈ
- `GET /models`: μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅

## π“ μ„±λ¥ λΉ„κµ

| ν•­λ© | ν„μ¬ test.py | FastAPI κΈ°λ° |
|------|-------------|-------------|
| λ™μ‹ μ‚¬μ©μ | 1λ… | λ‹¤μ |
| λ©”λ¨λ¦¬ μ‚¬μ© | λ†’μ | λ‚®μ |
| ν™•μ¥μ„± | μ ν•μ  | μ°μ |
| ν”λ«νΌ νΈν™μ„± | μ ν•μ  | μ°μ |
| μ—λ¬ μ²λ¦¬ | κΈ°λ³Έμ  | μ²΄κ³„μ  |

## π”§ μ„¤μ • μµμ…

`main.py`μ—μ„ λ‹¤μ μ„¤μ •μ„ λ³€κ²½ν•  μ μμµλ‹λ‹¤:

```python
MODEL_NAME = "mayo/llama-3-korean-bllossom-8b"  # λ¨λΈλ…
OLLAMA_API_URL = "http://localhost:11434/api/generate"  # Ollama API μ£Όμ†
```

## π¨ λ¬Έμ  ν•΄κ²°

1. **Ollama μ—°κ²° μ¤λ¥**
   - Ollamaκ°€ μ‹¤ν–‰ μ¤‘μΈμ§€ ν™•μΈ
   - `ollama serve` λ…λ Ήμ–΄λ΅ μ„λ²„ μ‹μ‘

2. **λ¨λΈ λ‹¤μ΄λ΅λ“ μ¤λ¥**
   - μΈν„°λ„· μ—°κ²° ν™•μΈ
   - `ollama pull mayo/llama-3-korean-bllossom-8b` μ¬μ‹¤ν–‰

3. **ν¬νΈ μ¶©λ**
   - `main.py`μ—μ„ ν¬νΈ λ²νΈ λ³€κ²½
   - λ‹¤λ¥Έ μ„λΉ„μ¤μ™€ ν¬νΈ μ¶©λ ν™•μΈ 